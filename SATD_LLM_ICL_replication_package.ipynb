{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SATD identification and classification by LLMs and in-context learning (RQ2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda-envs/llm_py310_torch2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset,Dataset,DatasetDict\n",
    "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import sentence_transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add context and prompt_context colums to the dataset\n",
    "def add_context(df, inputs):\n",
    "    context = [] # to be used in SentenceTransform\n",
    "    prompt_context = [] # to be used in prompt generation\n",
    "    for _,row in df.iterrows():\n",
    "        if inputs == 'ct':\n",
    "            if DATASET=='Maldonado62k':\n",
    "                context.append(row['comment_text'])\n",
    "                prompt_context.append('### Comment text: \"\"\" ' + row['comment_text'] + ' \"\"\"')\n",
    "            else:\n",
    "                context.append(row['comment_text'])\n",
    "                prompt_context.append('### Technical debt comment: \"\"\" ' + row['comment_text'] + ' \"\"\"')\n",
    "        elif inputs == 'fp+ct':\n",
    "            context.append(row['file_path'] + '\\n' + row['comment_text'])\n",
    "            prompt_context.append('### file path: ' + row['file_path'] + '\\n' +\n",
    "                       '### Technical debt comment: \"\"\" ' + row['comment_text'] + ' \"\"\"')\n",
    "        elif inputs == 'fp+cms+ct':\n",
    "            context.append(row['file_path'] + '\\n' + str(row['containing_method_signature']) + '\\n' + row['comment_text'])\n",
    "            prompt_context.append('### file path: ' + row['file_path'] + '\\n' +\n",
    "                       '### Containing method signature: \"\"\" ' + str(row['containing_method_signature']) + ' \"\"\"\\n' +\n",
    "                       '### Technical debt comment: \"\"\" ' + row['comment_text'] + ' \"\"\"')\n",
    "        elif inputs == 'fp+ct+cmb':\n",
    "            context.append(row['file_path'] + '\\n' + row['comment_text'] + '\\n' + str(row['containing_method']))\n",
    "            prompt_context.append('### file path: ' + row['file_path'] + '\\n' +\n",
    "                       '### Technical debt comment: \"\"\" ' + row['comment_text'] + ' \"\"\"\\n' +\n",
    "                       '### Containing method: \"\"\" ' + str(row['containing_method']).replace('\"\"\"',\"'''\") + ' \"\"\"')\n",
    "        else:\n",
    "            print('ERROR!')\n",
    "\n",
    "    df['context'] = context\n",
    "    df['prompt_context'] = prompt_context\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62275\n",
      "Index(['project_name', 'classification', 'comment_text', 'satd_orig', 'satd',\n",
      "       'satd_str'],\n",
      "      dtype='object')\n",
      "\n",
      "-------------- An example of input data - context ---------------\n",
      "\n",
      "// the generated classes must not be added in the generic JAR! // is that buggy on old JOnAS (2.4) ??\n",
      "\n",
      "-------------- An example of input data - prompt_context ---------------\n",
      "\n",
      "### Comment text: \"\"\" // the generated classes must not be added in the generic JAR! // is that buggy on old JOnAS (2.4) ?? \"\"\"\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'apache-ant-1.7.0': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 35728\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 4098\n",
       "     })\n",
       " }),\n",
       " 'apache-jmeter-2.10': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 34602\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 8057\n",
       "     })\n",
       " }),\n",
       " 'argouml': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 33149\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 9452\n",
       "     })\n",
       " }),\n",
       " 'columba-1.4-src': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 34541\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 6468\n",
       "     })\n",
       " }),\n",
       " 'emf-2.4.1': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 36016\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 4390\n",
       "     })\n",
       " }),\n",
       " 'hibernate-distribution-3.3.2.GA': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 36248\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 2968\n",
       "     })\n",
       " }),\n",
       " 'jEdit-4.2': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 34095\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 10322\n",
       "     })\n",
       " }),\n",
       " 'jfreechart-1.0.19': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 36203\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 4408\n",
       "     })\n",
       " }),\n",
       " 'jruby-1.4.0': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 35012\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 4897\n",
       "     })\n",
       " }),\n",
       " 'sql12': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 33896\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 7215\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read and prepare the Maldonado dataset (SATD identification)\n",
    "\n",
    "df = pd.read_csv('Dataset/Maldonado-62k/maldonado_corrected.csv')\n",
    "df['satd_str'] = df['satd'].apply(lambda x: 'SATD' if x == 1 else 'Not-SATD')\n",
    "print(len(df))\n",
    "print(df.columns)\n",
    "\n",
    "DATASET = 'Maldonado62k'\n",
    "\n",
    "INPUT = 'ct' # in Maldonado dataset there is only one input feature: comment text\n",
    "\n",
    "df = add_context(df, INPUT)\n",
    "\n",
    "print('\\n-------------- An example of input data - context ---------------\\n')\n",
    "print(df.context[4])\n",
    "print('\\n-------------- An example of input data - prompt_context ---------------\\n')\n",
    "print(df.prompt_context[4])\n",
    "print('-------------------------------------------------------\\n')\n",
    "\n",
    "df = df[['context','prompt_context','satd_str','project_name']]\n",
    "\n",
    "# for each project, split data to train and test and save it in a dataset\n",
    "dataset = {}\n",
    "for project_name in sorted(set(df['project_name'])):\n",
    "    test_df = df[df['project_name'] == project_name]\n",
    "    train_df = df[df['project_name'] != project_name]\n",
    "    train_df = train_df.drop_duplicates(subset='context') # remove duplicates from train\n",
    "    train_df = train_df.sample(frac=1, random_state=42) # shuffle train\n",
    "\n",
    "    data = DatasetDict({\"train\": Dataset.from_pandas(train_df), \"test\": Dataset.from_pandas(test_df)})\n",
    "    data=data.rename_column(\"satd_str\",\"label\")\n",
    "    data=data.remove_columns(['project_name','__index_level_0__'])\n",
    "    dataset[project_name] = data\n",
    "\n",
    "METRIC = 'accuracy'\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "789\n",
      "Index(['Unnamed: 0', 'dataset_id', 'repo_type', 'repo_name', 'filename',\n",
      "       'commit_introducing_revision', 'commit_removing_revision',\n",
      "       'comment_text', 'is_satd', 'satd_type', 'ml_satd_type',\n",
      "       'ml_satd_type_2', 'ml_pipeline_stage', 'file_content', 'satd_line',\n",
      "       'commit_message', 'containing_method', 'containing_method_signature',\n",
      "       'fold'],\n",
      "      dtype='object')\n",
      "\n",
      "-------------- An example of input data - context ---------------\n",
      "\n",
      "!TODO: An empty dictionary would actually also do here ... despite the fact that\n",
      "\n",
      "-------------- An example of input data - prompt_context ---------------\n",
      "\n",
      "### Technical debt comment: \"\"\" !TODO: An empty dictionary would actually also do here ... despite the fact that \"\"\"\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 710\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 79\n",
       "     })\n",
       " }),\n",
       " 1: DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 710\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 79\n",
       "     })\n",
       " }),\n",
       " 2: DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 710\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 79\n",
       "     })\n",
       " }),\n",
       " 3: DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 710\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 79\n",
       "     })\n",
       " }),\n",
       " 4: DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 711\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 78\n",
       "     })\n",
       " }),\n",
       " 5: DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 710\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 79\n",
       "     })\n",
       " }),\n",
       " 6: DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 710\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 79\n",
       "     })\n",
       " }),\n",
       " 7: DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 710\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 79\n",
       "     })\n",
       " }),\n",
       " 8: DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 710\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 79\n",
       "     })\n",
       " }),\n",
       " 9: DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 710\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['context', 'prompt_context', 'label'],\n",
       "         num_rows: 79\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read and prepare the OBrien dataset (SATD classification)\n",
    "\n",
    "# df = pd.read_csv('Dataset/23_Shades/OBrien_789.csv') # this version doesn't have the containing_method and containing_method_signature columns\n",
    "df = pd.read_csv('Dataset/23_Shades/OBrien_789_v2.csv')\n",
    "print(len(df))\n",
    "print(df.columns)\n",
    "\n",
    "df = df.rename(columns={\"filename\": \"file_path\"})\n",
    "\n",
    "DATASET = 'OBrien'\n",
    "\n",
    "INPUT = 'ct'               # only comment text\n",
    "# INPUT = 'fp+ct'            # file path + comment text\n",
    "# INPUT = 'fp+cms+ct'        # file path + containing method signature + comment text\n",
    "# INPUT = 'fp+ct+cmb'        # file path + comment text + containing method body\n",
    "\n",
    "df = add_context(df, INPUT)\n",
    "\n",
    "print('\\n-------------- An example of input data - context ---------------\\n')\n",
    "print(df.context[2]) # 151\n",
    "print('\\n-------------- An example of input data - prompt_context ---------------\\n')\n",
    "print(df.prompt_context[2]) # 151\n",
    "print('-------------------------------------------------------\\n')\n",
    "\n",
    "df = df[['context','prompt_context','satd_type','fold']]\n",
    "\n",
    "# for each project, split data to train and test and save it in a dataset\n",
    "dataset = {}\n",
    "for test_fold in sorted(set(df['fold'])):\n",
    "    test_df  = df[df['fold'] == test_fold]\n",
    "    train_df = df[df['fold'] != test_fold]        \n",
    "    train_df = train_df.sample(frac=1, random_state=42) # shuffle train\n",
    "\n",
    "    data = DatasetDict({\"train\": Dataset.from_pandas(train_df), \"test\": Dataset.from_pandas(test_df)})        \n",
    "    data=data.rename_column(\"satd_type\",\"label\")\n",
    "    data=data.remove_columns(['fold','__index_level_0__'])\n",
    "    dataset[test_fold] = data\n",
    "    \n",
    "METRIC = 'accuracy'\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:34<00:00,  6.81s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "checkpoint='google/flan-t5-xxl'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=5, do_sample=True, temperature=0.01) # when I set max_new_tokens=3, it generates 'Require' rather 'Requirement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(instruction: str, input_ctxt: str = None) -> str:\n",
    "    if input_ctxt:\n",
    "        return f\"\"\"\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_ctxt}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "\n",
    "def get_response(model, tokenizer, generation_config, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()} # we need to move the data to cuda if the model is on cuda\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            generation_config = generation_config\n",
    "        )[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confmat_str(real, pred, labels):\n",
    "    output = ''\n",
    "    cm = confusion_matrix(real, pred, labels=labels)\n",
    "    max_label_length = max([len(label) for label in labels] + [5])\n",
    "    output = \" \" * max_label_length + \" \" + \" \".join(label.ljust(max_label_length) for label in labels) + \"\\n\"\n",
    "    for i, label in enumerate(labels):\n",
    "        row = \" \".join([str(cm[i][j]).ljust(max_label_length) for j in range(len(labels))])\n",
    "        output += label.ljust(max_label_length) + \" \" + row + \"\\n\"\n",
    "    return output\n",
    "\n",
    "def split_to_tokens(text):\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_most_relevant_items_for_an_item(item_embed, items_embed, n):\n",
    "    cos_sim = sentence_transformers.util.cos_sim(item_embed, items_embed) # Note: make sure to pass ndarray not list due to performance\n",
    "    itemId_similarity = dict(zip(range(len(items_embed)),cos_sim.tolist()[0]))\n",
    "    itemId_similarity = dict(sorted(itemId_similarity.items(), key=lambda item: item[1], reverse=True)) # sort\n",
    "    itemId_similarity = [(k,itemId_similarity[k]) for k in list(itemId_similarity)[:n]] # take top n\n",
    "    return itemId_similarity\n",
    "\n",
    "def get_the_most_relevant_items_for_an_item_given_cos_sim(item_indx, cos_sim, n):\n",
    "    cos_sim_row = cos_sim[item_indx]\n",
    "    itemId_similarity = dict(zip(range(len(cos_sim_row)),cos_sim_row.tolist()))\n",
    "    itemId_similarity = dict(sorted(itemId_similarity.items(), key=lambda item: item[1], reverse=True)) # sort\n",
    "    itemId_similarity = [(k,itemId_similarity[k]) for k in list(itemId_similarity)[:n]] # take top n\n",
    "    return itemId_similarity\n",
    "\n",
    "   \n",
    "# Self-admitted technical debt (SATD) is technical debt admitted by the developer through source code comments. Assign a label of 1 to indicate SATD or 0 to indicate Not-SATD for each source code comment.\n",
    "\n",
    "init_prompt_for_Maldonado62k = \"\"\"\n",
    "Self-admitted technical debt (SATD) is technical debt admitted by the developer through source code comments. Assign the label of SATD or Not-SATD for each given source code comment.\n",
    "\n",
    "Here are some examples:\\n\\n\"\"\"\n",
    "\n",
    "# using the keywords in MAT paper\n",
    "init_prompt_for_Maldonado62k_MAT = \"\"\"\n",
    "Self-admitted technical debt (SATD) is technical debt admitted by the developer through source code comments. SATD comments usually contains specific keywords: TODO, FIXME, HACK, and XXX. Assign the label of SATD or Not-SATD for each given source code comment.\n",
    "\n",
    "Here are some examples:\\n\\n\"\"\"\n",
    "\n",
    "# using the keywords by Jitterbug paper (their Easy approach)\n",
    "init_prompt_for_Maldonado62k_Easy = \"\"\"\n",
    "Self-admitted technical debt (SATD) is technical debt admitted by the developer through source code comments. SATD comments usually contains specific keywords: TODO, FIXME, HACK, and WORKAROUND. Assign the label of SATD or Not-SATD for each given source code comment.\n",
    "\n",
    "Here are some examples:\\n\\n\"\"\"\n",
    "\n",
    "# using the keywords proposed by GPT4\n",
    "# prompt: \"what is the common keywords that developers use to highlight a code comment as self admitted technical debt.\"\n",
    "init_prompt_for_Maldonado62k_GPT4 = \"\"\"\n",
    "Self-admitted technical debt (SATD) is technical debt admitted by the developer through source code comments. SATD comments usually contains specific keywords: TODO, FIXME, HACK, XXX, NOTE, DEBT, REFACTOR, OPTIMIZE, TEMP, WORKAROUND, KLUDGE, REVIEW, NOFIX, PENDING, and BUG. Assign the label of SATD or Not-SATD for each given source code comment.\n",
    "\n",
    "Here are some examples:\\n\\n\"\"\"\n",
    "\n",
    "\n",
    "init_prompt_for_OBrien = \"\"\"\n",
    "There are six types of software technical debts:\n",
    "\n",
    "Requirement: Requirement debts can be functional or non-functional. In the functional case, implementations are left unfinished or in need of future feature support. In the non-functional case, the corresponding code does not meet the requirement standards (speed, memory usage, security, etc...).\n",
    "\n",
    "Code: Bad coding practices leading to poor legibility of code, making it difficult to understand and maintain.\n",
    "\n",
    "M&T: Problems found in implementations involving testing or monitoring subcomponents.\n",
    "\n",
    "Defect: Identified defects in the system that should be addressed.\n",
    "\n",
    "Design: Areas which violate good software design practices, causing poor flexibility to evolving business needs.\n",
    "\n",
    "Documentation: Inadequate documentation that exists within the software system. \n",
    "\n",
    "Here are some examples:\\n\\n\"\"\"\n",
    "\n",
    "\n",
    "def generate_prompt_without_adding_dynamic_examples(init_prompt, test_context):\n",
    "    prompt = init_prompt\n",
    "    prompt += test_context + '\\n'\n",
    "    prompt += '### Label: '\n",
    "    return prompt\n",
    "\n",
    "def generate_prompt_by_top_n_items(init_prompt, test_context, top_n_items, data):\n",
    "    prompt = init_prompt\n",
    "    for indx,similarity in top_n_items:\n",
    "        if len(split_to_tokens(prompt+data['prompt_context'][indx]+test_context))<500:\n",
    "            prompt += data['prompt_context'][indx] + '\\n'\n",
    "            prompt += '### Label: ' + data['label'][indx] + '\\n\\n'\n",
    "    prompt += test_context + '\\n'\n",
    "    prompt += '### Label: '\n",
    "    return prompt\n",
    "\n",
    "def generate_prompt_by_random_n_items(init_prompt, test_context, num_rand, data):\n",
    "    random_n_items = random.sample(range(len(data)), num_rand)\n",
    "    prompt = init_prompt\n",
    "    for indx in random_n_items:\n",
    "        if len(split_to_tokens(prompt+data['prompt_context'][indx]+test_context))<500:\n",
    "            prompt += data['prompt_context'][indx] + '\\n'\n",
    "            prompt += '### Label: ' + data['label'][indx] + '\\n\\n'\n",
    "    prompt += test_context + '\\n'\n",
    "    prompt += '### Label: '\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 23/23 [00:00<00:00, 151.56it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 186.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are six types of software technical debts:\n",
      "\n",
      "Requirement: Requirement debts can be functional or non-functional. In the functional case, implementations are left unfinished or in need of future feature support. In the non-functional case, the corresponding code does not meet the requirement standards (speed, memory usage, security, etc...).\n",
      "\n",
      "Code: Bad coding practices leading to poor legibility of code, making it difficult to understand and maintain.\n",
      "\n",
      "M&T: Problems found in implementations involving testing or monitoring subcomponents.\n",
      "\n",
      "Defect: Identified defects in the system that should be addressed.\n",
      "\n",
      "Design: Areas which violate good software design practices, causing poor flexibility to evolving business needs.\n",
      "\n",
      "Documentation: Inadequate documentation that exists within the software system. \n",
      "\n",
      "Here are some examples:\n",
      "\n",
      "### Technical debt comment: \"\"\" TODO normalice to make sum up to 1? \"\"\"\n",
      "### Label: Requirement\n",
      "\n",
      "### Technical debt comment: \"\"\" TODO: Set up self.batch_sum if self.bsum \"\"\"\n",
      "### Label: Requirement\n",
      "\n",
      "### Technical debt comment: \"\"\" TODO: ignore non-numerics \"\"\"\n",
      "### Label: Requirement\n",
      "\n",
      "### Technical debt comment: \"\"\" self.mpc_sum(3; -5) TODO: Future work: how to handle gracefully minus numbers \"\"\"\n",
      "### Label: \n"
     ]
    }
   ],
   "source": [
    "# show a prompt example that includes top n related items by SentenceTransformer\n",
    "\n",
    "n = 3 # number of examples in the prompt\n",
    "\n",
    "if DATASET == 'OBrien':\n",
    "    project_name = 9\n",
    "    indx = 11\n",
    "    init_prompt = init_prompt_for_OBrien\n",
    "else:\n",
    "    project_name = 'apache-ant-1.7.0'\n",
    "    indx = 2\n",
    "    init_prompt = init_prompt_for_Maldonado62k_MAT\n",
    "    \n",
    "st_model = SentenceTransformer('all-MiniLM-L6-v2') # model size: 80MB\n",
    "train_data_embed = st_model.encode(dataset[project_name]['train']['context'], show_progress_bar=True)\n",
    "test_data_embed = st_model.encode(dataset[project_name]['test']['context'], show_progress_bar=True)\n",
    "cos_sim = sentence_transformers.util.cos_sim(test_data_embed, train_data_embed)\n",
    "top_n_items = get_the_most_relevant_items_for_an_item_given_cos_sim(indx, cos_sim, n)\n",
    "print(generate_prompt_by_top_n_items(init_prompt, dataset[project_name]['test']['prompt_context'][indx], top_n_items, dataset[project_name]['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are six types of software technical debts:\n",
      "\n",
      "Requirement: Requirement debts can be functional or non-functional. In the functional case, implementations are left unfinished or in need of future feature support. In the non-functional case, the corresponding code does not meet the requirement standards (speed, memory usage, security, etc...).\n",
      "\n",
      "Code: Bad coding practices leading to poor legibility of code, making it difficult to understand and maintain.\n",
      "\n",
      "M&T: Problems found in implementations involving testing or monitoring subcomponents.\n",
      "\n",
      "Defect: Identified defects in the system that should be addressed.\n",
      "\n",
      "Design: Areas which violate good software design practices, causing poor flexibility to evolving business needs.\n",
      "\n",
      "Documentation: Inadequate documentation that exists within the software system. \n",
      "\n",
      "Here are some examples:\n",
      "\n",
      "### Technical debt comment: \"\"\" TODO: I believe this is not really much used \"\"\"\n",
      "### Label: Code\n",
      "\n",
      "### Technical debt comment: \"\"\" TODO In future; need to update action to handle (continuous) DELTA buttons using gym's Box space \"\"\"\n",
      "### Label: Requirement\n",
      "\n",
      "### Technical debt comment: \"\"\" TODO add checking to make sure number of keys in h5 file matches number of lines in csv file \"\"\"\n",
      "### Label: Requirement\n",
      "\n",
      "### Technical debt comment: \"\"\" self.mpc_sum(3; -5) TODO: Future work: how to handle gracefully minus numbers \"\"\"\n",
      "### Label: \n"
     ]
    }
   ],
   "source": [
    "# show a prompt example that includes n random items\n",
    "\n",
    "n = 3 # number of examples in the prompt\n",
    "\n",
    "if DATASET == 'OBrien':\n",
    "    project_name = 9\n",
    "    indx = 11\n",
    "    init_prompt = init_prompt_for_OBrien\n",
    "else:\n",
    "    project_name = 'apache-ant-1.7.0'\n",
    "    indx = 2\n",
    "    init_prompt = init_prompt_for_Maldonado62k_MAT\n",
    "    \n",
    "print(generate_prompt_by_random_n_items(init_prompt, dataset[project_name]['test']['prompt_context'][indx], n, dataset[project_name]['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the experiments for num_instances= [0, 1, 2, 3, 5, 10, 15, 20]\n",
      "Run the experiments for: OBrien_Input-ct_flan-t5-xxl_ICL-nearest-00\n",
      "---------- 0 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (717 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.429     0.375     0.400        24\n",
      "       Defect      0.250     0.250     0.250         4\n",
      "       Design      0.429     0.333     0.375         9\n",
      "Documentation      0.000     0.000     0.000         0\n",
      "          M&T      0.250     0.286     0.267         7\n",
      "  Requirement      0.636     0.600     0.618        35\n",
      "\n",
      "     accuracy                          0.456        79\n",
      "    macro avg      0.332     0.307     0.318        79\n",
      " weighted avg      0.496     0.456     0.474        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   21            6             4             2             2             0            \n",
      "Code          8             9             2             1             1             3            \n",
      "M&T           3             0             2             0             2             0            \n",
      "Design        1             4             0             3             1             0            \n",
      "Documentation 0             0             0             0             0             0            \n",
      "Defect        0             2             0             1             0             1            \n",
      "\n",
      "---------- 1 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.200     0.444     0.276         9\n",
      "       Defect      0.667     0.200     0.308        10\n",
      "       Design      0.500     0.182     0.267        11\n",
      "Documentation      0.200     0.500     0.286         2\n",
      "          M&T      0.533     1.000     0.696         8\n",
      "  Requirement      0.750     0.615     0.676        39\n",
      "\n",
      "     accuracy                          0.519        79\n",
      "    macro avg      0.475     0.490     0.418        79\n",
      " weighted avg      0.606     0.519     0.519        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   24            9             1             2             2             1            \n",
      "Code          1             4             2             0             2             0            \n",
      "M&T           0             0             8             0             0             0            \n",
      "Design        2             5             2             2             0             0            \n",
      "Documentation 1             0             0             0             1             0            \n",
      "Defect        4             2             2             0             0             2            \n",
      "\n",
      "---------- 2 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.391     0.529     0.450        17\n",
      "       Defect      0.000     0.000     0.000        10\n",
      "       Design      0.500     0.077     0.133        13\n",
      "Documentation      0.167     1.000     0.286         1\n",
      "          M&T      0.364     0.444     0.400         9\n",
      "  Requirement      0.556     0.690     0.615        29\n",
      "\n",
      "     accuracy                          0.443        79\n",
      "    macro avg      0.330     0.457     0.314        79\n",
      " weighted avg      0.414     0.443     0.394        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   20            7             1             0             1             0            \n",
      "Code          4             9             1             1             2             0            \n",
      "M&T           4             0             4             0             0             1            \n",
      "Design        4             6             0             1             2             0            \n",
      "Documentation 0             0             0             0             1             0            \n",
      "Defect        4             1             5             0             0             0            \n",
      "\n",
      "---------- 3 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.538     0.609     0.571        23\n",
      "       Defect      1.000     0.357     0.526        14\n",
      "       Design      0.000     0.000     0.000         7\n",
      "Documentation      0.600     1.000     0.750         3\n",
      "          M&T      0.545     0.857     0.667         7\n",
      "  Requirement      0.567     0.680     0.618        25\n",
      "\n",
      "     accuracy                          0.570        79\n",
      "    macro avg      0.542     0.584     0.522        79\n",
      " weighted avg      0.584     0.570     0.543        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   17            5             1             1             1             0            \n",
      "Code          6             14            1             1             1             0            \n",
      "M&T           1             0             6             0             0             0            \n",
      "Design        3             4             0             0             0             0            \n",
      "Documentation 0             0             0             0             3             0            \n",
      "Defect        3             3             3             0             0             5            \n",
      "\n",
      "---------- 4 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.652     0.536     0.588        28\n",
      "       Defect      0.000     0.000     0.000         4\n",
      "       Design      0.500     0.333     0.400         6\n",
      "Documentation      0.222     1.000     0.364         2\n",
      "          M&T      0.417     1.000     0.588         5\n",
      "  Requirement      0.786     0.667     0.721        33\n",
      "\n",
      "     accuracy                          0.590        78\n",
      "    macro avg      0.429     0.589     0.444        78\n",
      " weighted avg      0.637     0.590     0.594        78\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   22            4             1             1             4             1            \n",
      "Code          3             15            6             1             2             1            \n",
      "M&T           0             0             5             0             0             0            \n",
      "Design        1             3             0             2             0             0            \n",
      "Documentation 0             0             0             0             2             0            \n",
      "Defect        2             1             0             0             1             0            \n",
      "\n",
      "---------- 5 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.450     0.474     0.462        19\n",
      "       Defect      0.750     0.500     0.600         6\n",
      "       Design      0.400     0.250     0.308         8\n",
      "Documentation      0.000     0.000     0.000         1\n",
      "          M&T      0.444     0.500     0.471         8\n",
      "  Requirement      0.676     0.676     0.676        37\n",
      "\n",
      "     accuracy                          0.544        79\n",
      "    macro avg      0.453     0.400     0.419        79\n",
      " weighted avg      0.567     0.544     0.552        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   25            4             3             2             3             0            \n",
      "Code          6             9             1             1             1             1            \n",
      "M&T           2             2             4             0             0             0            \n",
      "Design        1             4             1             2             0             0            \n",
      "Documentation 1             0             0             0             0             0            \n",
      "Defect        2             1             0             0             0             3            \n",
      "\n",
      "---------- 6 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.474     0.391     0.429        23\n",
      "       Defect      0.667     0.286     0.400         7\n",
      "       Design      0.200     0.333     0.250         3\n",
      "Documentation      0.400     0.667     0.500         3\n",
      "          M&T      0.400     0.600     0.480        10\n",
      "  Requirement      0.625     0.606     0.615        33\n",
      "\n",
      "     accuracy                          0.506        79\n",
      "    macro avg      0.461     0.481     0.446        79\n",
      " weighted avg      0.531     0.506     0.507        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   20            7             3             2             1             0            \n",
      "Code          5             9             5             2             2             0            \n",
      "M&T           3             1             6             0             0             0            \n",
      "Design        0             1             0             1             0             1            \n",
      "Documentation 1             0             0             0             2             0            \n",
      "Defect        3             1             1             0             0             2            \n",
      "\n",
      "---------- 7 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.250     0.278     0.263        18\n",
      "       Defect      0.375     0.429     0.400         7\n",
      "       Design      0.333     0.100     0.154        10\n",
      "Documentation      0.000     0.000     0.000         0\n",
      "          M&T      0.556     0.833     0.667        12\n",
      "  Requirement      0.577     0.469     0.517        32\n",
      "\n",
      "     accuracy                          0.430        79\n",
      "    macro avg      0.348     0.351     0.333        79\n",
      " weighted avg      0.450     0.430     0.426        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   15            9             3             1             3             1            \n",
      "Code          4             5             4             0             1             4            \n",
      "M&T           0             1             10            1             0             0            \n",
      "Design        5             4             0             1             0             0            \n",
      "Documentation 0             0             0             0             0             0            \n",
      "Defect        2             1             1             0             0             3            \n",
      "\n",
      "---------- 8 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.667     0.522     0.585        23\n",
      "       Defect      1.000     0.200     0.333        10\n",
      "       Design      0.333     0.300     0.316        10\n",
      "Documentation      0.000     0.000     0.000         0\n",
      "          M&T      0.389     0.778     0.519         9\n",
      "  Requirement      0.581     0.667     0.621        27\n",
      "\n",
      "     accuracy                          0.532        79\n",
      "    macro avg      0.495     0.411     0.396        79\n",
      " weighted avg      0.606     0.532     0.524        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   18            3             3             3             0             0            \n",
      "Code          5             12            4             2             0             0            \n",
      "M&T           2             0             7             0             0             0            \n",
      "Design        4             1             2             3             0             0            \n",
      "Documentation 0             0             0             0             0             0            \n",
      "Defect        2             2             2             1             1             2            \n",
      "\n",
      "---------- 9 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.643     0.391     0.486        23\n",
      "       Defect      0.333     0.100     0.154        10\n",
      "       Design      0.250     0.333     0.286         3\n",
      "Documentation      0.200     1.000     0.333         3\n",
      "          M&T      0.235     0.444     0.308         9\n",
      "  Requirement      0.692     0.581     0.632        31\n",
      "\n",
      "     accuracy                          0.456        79\n",
      "    macro avg      0.392     0.475     0.366        79\n",
      " weighted avg      0.545     0.456     0.468        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   18            2             5             1             4             1            \n",
      "Code          4             9             4             1             4             1            \n",
      "M&T           2             1             4             0             2             0            \n",
      "Design        0             1             0             1             1             0            \n",
      "Documentation 0             0             0             0             3             0            \n",
      "Defect        2             1             4             1             1             1            \n",
      "\n",
      "=========== Overall ==========\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.466     0.459     0.462       207\n",
      "       Defect      0.543     0.232     0.325        82\n",
      "       Design      0.356     0.200     0.256        80\n",
      "Documentation      0.200     0.800     0.320        15\n",
      "          M&T      0.418     0.667     0.514        84\n",
      "  Requirement      0.643     0.623     0.633       321\n",
      "\n",
      "     accuracy                          0.504       789\n",
      "    macro avg      0.438     0.497     0.418       789\n",
      " weighted avg      0.525     0.504     0.499       789\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   200           56            25            15            21            4            \n",
      "Code          46            95            30            10            16            10           \n",
      "M&T           17            5             56            1             4             1            \n",
      "Design        21            33            5             16            4             1            \n",
      "Documentation 3             0             0             0             12            0            \n",
      "Defect        24            15            18            3             3             19           \n",
      "\n",
      "\n",
      "Number of unrecognized predictions: 13 \n",
      "We considered them as the majority class.\n",
      "Run the experiments for num_instances= [0, 1, 2, 3, 5, 10, 15, 20]\n",
      "Run the experiments for: OBrien_Input-ct_flan-t5-xxl_ICL-nearest-01\n",
      "---------- 0 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.636     0.292     0.400        24\n",
      "       Defect      0.333     0.500     0.400         4\n",
      "       Design      0.333     0.444     0.381         9\n",
      "Documentation      0.000     0.000     0.000         0\n",
      "          M&T      0.500     0.571     0.533         7\n",
      "  Requirement      0.600     0.686     0.640        35\n",
      "\n",
      "     accuracy                          0.519        79\n",
      "    macro avg      0.401     0.416     0.392        79\n",
      " weighted avg      0.558     0.519     0.516        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   24            3             3             4             1             0            \n",
      "Code          11            7             1             2             0             3            \n",
      "M&T           3             0             4             0             0             0            \n",
      "Design        2             1             0             4             1             1            \n",
      "Documentation 0             0             0             0             0             0            \n",
      "Defect        0             0             0             2             0             2            \n",
      "\n",
      "---------- 1 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.333     0.444     0.381         9\n",
      "       Defect      0.667     0.200     0.308        10\n",
      "       Design      0.500     0.273     0.353        11\n",
      "Documentation      1.000     0.500     0.667         2\n",
      "          M&T      0.400     0.750     0.522         8\n",
      "  Requirement      0.619     0.667     0.642        39\n",
      "\n",
      "     accuracy                          0.532        79\n",
      "    macro avg      0.587     0.472     0.479        79\n",
      " weighted avg      0.563     0.532     0.518        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   26            4             5             3             0             1            \n",
      "Code          3             4             2             0             0             0            \n",
      "M&T           2             0             6             0             0             0            \n",
      "Design        4             4             0             3             0             0            \n",
      "Documentation 1             0             0             0             1             0            \n",
      "Defect        6             0             2             0             0             2            \n",
      "\n",
      "---------- 2 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.500     0.412     0.452        17\n",
      "       Defect      0.600     0.300     0.400        10\n",
      "       Design      0.286     0.154     0.200        13\n",
      "Documentation      0.333     1.000     0.500         1\n",
      "          M&T      0.429     0.333     0.375         9\n",
      "  Requirement      0.512     0.759     0.611        29\n",
      "\n",
      "     accuracy                          0.481        79\n",
      "    macro avg      0.443     0.493     0.423        79\n",
      " weighted avg      0.471     0.481     0.454        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   22            3             1             2             0             1            \n",
      "Code          8             7             0             2             0             0            \n",
      "M&T           4             0             3             1             0             1            \n",
      "Design        5             4             0             2             2             0            \n",
      "Documentation 0             0             0             0             1             0            \n",
      "Defect        4             0             3             0             0             3            \n",
      "\n",
      "---------- 3 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.600     0.522     0.558        23\n",
      "       Defect      1.000     0.357     0.526        14\n",
      "       Design      0.000     0.000     0.000         7\n",
      "Documentation      0.333     0.333     0.333         3\n",
      "          M&T      0.583     1.000     0.737         7\n",
      "  Requirement      0.543     0.760     0.633        25\n",
      "\n",
      "     accuracy                          0.557        79\n",
      "    macro avg      0.510     0.495     0.465        79\n",
      " weighted avg      0.588     0.557     0.534        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   19            3             1             1             1             0            \n",
      "Code          7             12            1             2             1             0            \n",
      "M&T           0             0             7             0             0             0            \n",
      "Design        4             3             0             0             0             0            \n",
      "Documentation 1             0             1             0             1             0            \n",
      "Defect        4             2             2             1             0             5            \n",
      "\n",
      "---------- 4 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.538     0.250     0.341        28\n",
      "       Defect      0.000     0.000     0.000         4\n",
      "       Design      0.300     0.500     0.375         6\n",
      "Documentation      0.222     1.000     0.364         2\n",
      "          M&T      0.500     1.000     0.667         5\n",
      "  Requirement      0.719     0.697     0.708        33\n",
      "\n",
      "     accuracy                          0.513        78\n",
      "    macro avg      0.380     0.574     0.409        78\n",
      " weighted avg      0.558     0.513     0.503        78\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   23            3             1             3             1             2            \n",
      "Code          8             7             3             3             5             2            \n",
      "M&T           0             0             5             0             0             0            \n",
      "Design        1             1             1             3             0             0            \n",
      "Documentation 0             0             0             0             2             0            \n",
      "Defect        0             2             0             1             1             0            \n",
      "\n",
      "---------- 5 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.500     0.421     0.457        19\n",
      "       Defect      0.750     0.500     0.600         6\n",
      "       Design      0.500     0.500     0.500         8\n",
      "Documentation      0.000     0.000     0.000         1\n",
      "          M&T      0.444     0.500     0.471         8\n",
      "  Requirement      0.641     0.676     0.658        37\n",
      "\n",
      "     accuracy                          0.557        79\n",
      "    macro avg      0.473     0.433     0.448        79\n",
      " weighted avg      0.573     0.557     0.562        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   25            3             3             3             3             0            \n",
      "Code          8             8             2             1             0             0            \n",
      "M&T           2             2             4             0             0             0            \n",
      "Design        2             2             0             4             0             0            \n",
      "Documentation 0             0             0             0             0             1            \n",
      "Defect        2             1             0             0             0             3            \n",
      "\n",
      "---------- 6 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.615     0.348     0.444        23\n",
      "       Defect      0.600     0.429     0.500         7\n",
      "       Design      0.167     0.333     0.222         3\n",
      "Documentation      0.667     0.667     0.667         3\n",
      "          M&T      0.636     0.700     0.667        10\n",
      "  Requirement      0.585     0.727     0.649        33\n",
      "\n",
      "     accuracy                          0.570        79\n",
      "    macro avg      0.545     0.534     0.525        79\n",
      " weighted avg      0.589     0.570     0.563        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   24            5             2             2             0             0            \n",
      "Code          9             8             1             3             1             1            \n",
      "M&T           3             0             7             0             0             0            \n",
      "Design        1             0             0             1             0             1            \n",
      "Documentation 1             0             0             0             2             0            \n",
      "Defect        3             0             1             0             0             3            \n",
      "\n",
      "---------- 7 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.400     0.333     0.364        18\n",
      "       Defect      0.444     0.571     0.500         7\n",
      "       Design      0.500     0.300     0.375        10\n",
      "Documentation      0.000     0.000     0.000         0\n",
      "          M&T      0.533     0.667     0.593        12\n",
      "  Requirement      0.697     0.719     0.708        32\n",
      "\n",
      "     accuracy                          0.557        79\n",
      "    macro avg      0.429     0.432     0.423        79\n",
      " weighted avg      0.557     0.557     0.551        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   23            4             1             1             1             2            \n",
      "Code          5             6             4             1             0             2            \n",
      "M&T           0             2             8             1             0             1            \n",
      "Design        3             3             1             3             0             0            \n",
      "Documentation 0             0             0             0             0             0            \n",
      "Defect        2             0             1             0             0             4            \n",
      "\n",
      "---------- 8 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.692     0.391     0.500        23\n",
      "       Defect      0.833     0.500     0.625        10\n",
      "       Design      0.273     0.300     0.286        10\n",
      "Documentation      0.000     0.000     0.000         0\n",
      "          M&T      0.545     0.667     0.600         9\n",
      "  Requirement      0.568     0.778     0.656        27\n",
      "\n",
      "     accuracy                          0.557        79\n",
      "    macro avg      0.485     0.439     0.444        79\n",
      " weighted avg      0.598     0.557     0.553        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   21            3             1             1             0             1            \n",
      "Code          5             9             3             6             0             0            \n",
      "M&T           3             0             6             0             0             0            \n",
      "Design        6             1             0             3             0             0            \n",
      "Documentation 0             0             0             0             0             0            \n",
      "Defect        2             0             1             1             1             5            \n",
      "\n",
      "---------- 9 ----------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.750     0.391     0.514        23\n",
      "       Defect      0.000     0.000     0.000        10\n",
      "       Design      0.167     0.333     0.222         3\n",
      "Documentation      0.300     1.000     0.462         3\n",
      "          M&T      0.333     0.556     0.417         9\n",
      "  Requirement      0.559     0.613     0.585        31\n",
      "\n",
      "     accuracy                          0.468        79\n",
      "    macro avg      0.351     0.482     0.367        79\n",
      " weighted avg      0.493     0.468     0.453        79\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   19            2             5             2             2             1            \n",
      "Code          5             9             3             2             3             1            \n",
      "M&T           4             0             5             0             0             0            \n",
      "Design        0             1             0             1             1             0            \n",
      "Documentation 0             0             0             0             3             0            \n",
      "Defect        6             0             2             1             1             0            \n",
      "\n",
      "=========== Overall ==========\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         Code      0.554     0.372     0.445       207\n",
      "       Defect      0.551     0.329     0.412        82\n",
      "       Design      0.316     0.300     0.308        80\n",
      "Documentation      0.278     0.667     0.392        15\n",
      "          M&T      0.487     0.655     0.558        84\n",
      "  Requirement      0.601     0.704     0.648       321\n",
      "\n",
      "     accuracy                          0.531       789\n",
      "    macro avg      0.464     0.504     0.461       789\n",
      " weighted avg      0.536     0.531     0.522       789\n",
      "\n",
      "              Requirement   Code          M&T           Design        Documentation Defect       \n",
      "Requirement   226           33            23            22            9             8            \n",
      "Code          69            77            20            22            10            9            \n",
      "M&T           21            4             55            2             0             2            \n",
      "Design        28            20            2             24            4             2            \n",
      "Documentation 3             0             1             0             10            1            \n",
      "Defect        29            5             12            6             3             27           \n",
      "\n",
      "\n",
      "Number of unrecognized predictions: 1 \n",
      "We considered them as the majority class.\n",
      "Run the experiments for num_instances= [0, 1, 2, 3, 5, 10, 15, 20]\n",
      "Run the experiments for: OBrien_Input-ct_flan-t5-xxl_ICL-nearest-02\n",
      "---------- 0 ----------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">82</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 79 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">'ERROR!'</span>)                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 80 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#print(len(split_to_tokens(prompt)))</span>                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 81 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(split_to_tokens(prompt))&lt;<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1000</span>:                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 82 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>pred = get_response(model, tokenizer, generation_config, prompt)       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 83 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 84 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>pred = <span style=\"color: #808000; text-decoration-color: #808000\">''</span>                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 85 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> label <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> labels:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_response</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">24</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>inputs = tokenizer(prompt, return_tensors=<span style=\"color: #808000; text-decoration-color: #808000\">'pt'</span>)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>inputs = {key: value.to(device) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> key, value <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> inputs.items()} <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># we need to move </span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">23 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>output = tokenizer.decode(                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>24 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>model.generate(                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>inputs[<span style=\"color: #808000; text-decoration-color: #808000\">\"input_ids\"</span>],                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>generation_config = generation_config                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>],                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jovyan/conda-envs/llm_py310_torch2/lib/python3.10/site-packages/torch/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_contextlib.py</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> :<span style=\"color: #0000ff; text-decoration-color: #0000ff\">115</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@functools</span>.wraps(func)                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>(*args, **kwargs):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> ctx_factory():                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>115 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func(*args, **kwargs)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> decorate_context                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jovyan/conda-envs/llm_py310_torch2/lib/python3.10/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ut</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1557</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1554 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>logits_warper = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._get_logits_warper(generation_config)                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1555 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1556 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># 12. expand input_ids with `num_return_sequences` additional sequences per </span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1557 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>input_ids, model_kwargs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._expand_inputs_for_generation(                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1558 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>input_ids=input_ids,                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1559 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>expand_size=generation_config.num_return_sequences,                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1560 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>is_encoder_decoder=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.is_encoder_decoder,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jovyan/conda-envs/llm_py310_torch2/lib/python3.10/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ut</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">719</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_expand_inputs_for_generation</span>                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 716 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> dict_to_expand                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 717 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 718 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> input_ids <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 719 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>input_ids = input_ids.repeat_interleave(expand_size, dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>)                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 720 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 721 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>model_kwargs = _expand_dict_for_generation(model_kwargs)                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 722 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m82\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 79 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m'\u001b[0m\u001b[33mERROR!\u001b[0m\u001b[33m'\u001b[0m)                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 80 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m#print(len(split_to_tokens(prompt)))\u001b[0m                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 81 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(split_to_tokens(prompt))<\u001b[94m1000\u001b[0m:                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 82 \u001b[2m│   │   │   │   │   \u001b[0mpred = get_response(model, tokenizer, generation_config, prompt)       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 83 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 84 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mpred = \u001b[33m'\u001b[0m\u001b[33m'\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 85 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m label \u001b[95min\u001b[0m labels:                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mget_response\u001b[0m:\u001b[94m24\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m│   \u001b[0minputs = tokenizer(prompt, return_tensors=\u001b[33m'\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m'\u001b[0m)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m│   \u001b[0minputs = {key: value.to(device) \u001b[94mfor\u001b[0m key, value \u001b[95min\u001b[0m inputs.items()} \u001b[2m# we need to move \u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   \u001b[0moutput = tokenizer.decode(                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m24 \u001b[2m│   │   \u001b[0mmodel.generate(                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m\u001b[2m│   │   │   \u001b[0minputs[\u001b[33m\"\u001b[0m\u001b[33minput_ids\u001b[0m\u001b[33m\"\u001b[0m],                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m│   │   │   \u001b[0mgeneration_config = generation_config                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m27 \u001b[0m\u001b[2m│   │   \u001b[0m)[\u001b[94m0\u001b[0m],                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/jovyan/conda-envs/llm_py310_torch2/lib/python3.10/site-packages/torch/utils/\u001b[0m\u001b[1;33m_contextlib.py\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m :\u001b[94m115\u001b[0m in \u001b[92mdecorate_context\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@functools\u001b[0m.wraps(func)                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdecorate_context\u001b[0m(*args, **kwargs):                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m ctx_factory():                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m115 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m decorate_context                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/jovyan/conda-envs/llm_py310_torch2/lib/python3.10/site-packages/transformers/generation/\u001b[0m\u001b[1;33mut\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mils.py\u001b[0m:\u001b[94m1557\u001b[0m in \u001b[92mgenerate\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1554 \u001b[0m\u001b[2m│   │   │   \u001b[0mlogits_warper = \u001b[96mself\u001b[0m._get_logits_warper(generation_config)                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1555 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1556 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# 12. expand input_ids with `num_return_sequences` additional sequences per \u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1557 \u001b[2m│   │   │   \u001b[0minput_ids, model_kwargs = \u001b[96mself\u001b[0m._expand_inputs_for_generation(                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1558 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minput_ids=input_ids,                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1559 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mexpand_size=generation_config.num_return_sequences,                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1560 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mis_encoder_decoder=\u001b[96mself\u001b[0m.config.is_encoder_decoder,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/jovyan/conda-envs/llm_py310_torch2/lib/python3.10/site-packages/transformers/generation/\u001b[0m\u001b[1;33mut\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mils.py\u001b[0m:\u001b[94m719\u001b[0m in \u001b[92m_expand_inputs_for_generation\u001b[0m                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 716 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m dict_to_expand                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 717 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 718 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m input_ids \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 719 \u001b[2m│   │   │   \u001b[0minput_ids = input_ids.repeat_interleave(expand_size, dim=\u001b[94m0\u001b[0m)                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 720 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 721 \u001b[0m\u001b[2m│   │   \u001b[0mmodel_kwargs = _expand_dict_for_generation(model_kwargs)                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 722 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_scheduler\n",
    "from datasets import load_metric\n",
    "\n",
    "# ICL_METHOD = 'task-level' # use the same prompt (zero-shot or the same demonstration examples) for all test data\n",
    "ICL_METHOD = 'instance-level-nearest' # use different prompts (selects different demonstration examples) for different test samples by nearest examples selection\n",
    "# ICL_METHOD = 'instance-level-random' # use different prompts (selects different demonstration examples) for different test samples by random selection\n",
    "\n",
    "if DATASET == 'OBrien':\n",
    "    INIT_PROMPT = init_prompt_for_OBrien\n",
    "    # INIT_PROMPT = \"\" # provide no description for the task (i.e., just provide some examples)\n",
    "elif DATASET == 'Maldonado62k':\n",
    "    # INIT_PROMPT = init_prompt_for_Maldonado62k # include no keywords\n",
    "    INIT_PROMPT = init_prompt_for_Maldonado62k_MAT # include MAT keywords\n",
    "    # INIT_PROMPT = init_prompt_for_Maldonado62k_Easy # include Easy keywords\n",
    "    # INIT_PROMPT = init_prompt_for_Maldonado62k_GPT4 # include GPT4 keywords\n",
    "else:\n",
    "    print(\"ERROR! Unknown dataset\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "st_model = SentenceTransformer('all-MiniLM-L6-v2') # model size: 80MB\n",
    "\n",
    "if 'instance' in ICL_METHOD:\n",
    "    if len(INIT_PROMPT)>0:\n",
    "        num_instances = [0,1,2,3,5,10,15,20]\n",
    "    else:\n",
    "        num_instances = [1,2,3,5,10,15,20]\n",
    "else:\n",
    "    num_instances = [0]\n",
    "\n",
    "for NUM_EXAMPLES_IN_PROMPT in num_instances:\n",
    "    print('Run the experiments for num_instances=',num_instances)\n",
    "\n",
    "    if ICL_METHOD == 'task-level':\n",
    "        icl_name = '_ICL-task'\n",
    "    elif ICL_METHOD == 'instance-level-nearest':\n",
    "        icl_name = '_ICL-nearest-' + str(NUM_EXAMPLES_IN_PROMPT).zfill(2)\n",
    "    elif ICL_METHOD == 'instance-level-random':\n",
    "        icl_name = '_ICL-random-' + str(NUM_EXAMPLES_IN_PROMPT).zfill(2)\n",
    "    else:\n",
    "        icl_name = '_ICL-error'\n",
    "\n",
    "    print('Run the experiments for: ' + DATASET + '_Input-' + INPUT + '_' + checkpoint.split('/')[-1] + icl_name)\n",
    "    file_name = 'Adding_Custom_Layers_Results/' + DATASET + '_Input-' + INPUT + '_' + checkpoint.split('/')[-1] + icl_name\n",
    "\n",
    "    test_results = {}\n",
    "    projects_real = {}\n",
    "    projects_pred = {}\n",
    "    all_real = []\n",
    "    all_pred = []\n",
    "    all_context = []\n",
    "    all_project = []\n",
    "    labels = list(set(dataset[project_name]['train']['label']))\n",
    "\n",
    "    unrecognized_pred = 0 # don't move it to outer loop\n",
    "    with open(file_name+'_confmat.txt', \"w\") as output_file:\n",
    "        for project_name, data in dataset.items():\n",
    "            print('---------- '+str(project_name)+' ----------')\n",
    "            output_file.write('\\n---------- '+str(project_name)+' ----------\\n')\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            if True: # ICL_METHOD == 'instance-level-nearest': \n",
    "                train_data_embed = st_model.encode(data['train']['context'], show_progress_bar=False)\n",
    "                test_data_embed = st_model.encode(data['test']['context'], show_progress_bar=False)\n",
    "                cos_sim = sentence_transformers.util.cos_sim(test_data_embed, train_data_embed)\n",
    "            test_results[project_name] = []\n",
    "            projects_real[project_name] = []\n",
    "            projects_pred[project_name] = []\n",
    "            for indx, row, row_embed in zip(range(len(data['test'])), data['test'], test_data_embed):\n",
    "                if ICL_METHOD == 'task-level':\n",
    "                    # prompt = generate_prompt(INIT_PROMPT, row['prompt_context'])\n",
    "                    prompt = generate_prompt_without_adding_dynamic_examples(INIT_PROMPT, row['prompt_context'])\n",
    "                elif ICL_METHOD == 'instance-level-nearest':\n",
    "                    top_n_items = get_the_most_relevant_items_for_an_item_given_cos_sim(indx, cos_sim, NUM_EXAMPLES_IN_PROMPT)\n",
    "                    prompt = generate_prompt_by_top_n_items(INIT_PROMPT, row['prompt_context'], top_n_items, data['train'])\n",
    "                elif ICL_METHOD == 'instance-level-random':\n",
    "                    prompt = generate_prompt_by_random_n_items(INIT_PROMPT, row['prompt_context'], NUM_EXAMPLES_IN_PROMPT, data['train'])\n",
    "                else:\n",
    "                    print('ERROR!')\n",
    "                #print(len(split_to_tokens(prompt)))\n",
    "                if len(split_to_tokens(prompt))<1000:\n",
    "                    pred = get_response(model, tokenizer, generation_config, prompt)\n",
    "                else:\n",
    "                    pred = ''\n",
    "                for label in labels:\n",
    "                    if len(pred)>0 and pred.split()[0].lower() == label.lower():\n",
    "                        pred = label\n",
    "                if pred not in labels:\n",
    "                    #print(pred)\n",
    "                    if DATASET=='Maldonado62k':\n",
    "                        pred = 'Not-SATD'\n",
    "                    elif DATASET=='OBrien':\n",
    "                        pred = 'Requirement'\n",
    "                    unrecognized_pred += 1\n",
    "                if pred=='SATD' and row['label']=='Not-SATD' and False:\n",
    "                    print(prompt)\n",
    "                    print('--------------------------')\n",
    "                projects_real[project_name].append(row['label'])               \n",
    "                projects_pred[project_name].append(pred)\n",
    "                all_context.append(row['prompt_context'])\n",
    "                all_project.append(project_name)\n",
    "            all_real += projects_real[project_name]    \n",
    "            all_pred += projects_pred[project_name]\n",
    "            # print precision recall and F1 for this project\n",
    "            print(classification_report(projects_real[project_name], projects_pred[project_name], zero_division=0, digits=3))\n",
    "            output_file.write(classification_report(projects_real[project_name], projects_pred[project_name], zero_division=0, digits=3)+\"\\n\")\n",
    "            # print confusion matrix for this project\n",
    "            confmat_str = get_confmat_str(projects_real[project_name], projects_pred[project_name], labels=labels)\n",
    "            print(confmat_str)\n",
    "            output_file.write(confmat_str)\n",
    "        print('=========== Overall ==========')\n",
    "        output_file.write('\\n=========== Overall ==========\\n')\n",
    "        # print precision recall and F1 for all data\n",
    "        print(classification_report(all_real, all_pred, zero_division=0, digits=3))\n",
    "        output_file.write(classification_report(all_real, all_pred, zero_division=0, digits=3)+\"\\n\")\n",
    "        # print confusion matrix for all data\n",
    "        confmat_str = get_confmat_str(all_real, all_pred, labels=labels)\n",
    "        print(confmat_str)\n",
    "        output_file.write(confmat_str)\n",
    "        print('\\nNumber of unrecognized predictions:', unrecognized_pred, '\\nWe considered them as the majority class.')\n",
    "        output_file.write('\\nNumber of unrecognized predictions: '+str(unrecognized_pred)+'\\nWe considered them as the majority class.\\n')\n",
    "\n",
    "    test_result_df = pd.DataFrame({'project': all_project, 'context':all_context, 'real': all_real, 'pred': all_pred})\n",
    "    test_result_df.to_csv(file_name+'_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62275\n",
      "Index(['project', 'context', 'real', 'pred'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apache-jmeter-2.10</td>\n",
       "      <td>0.801444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>argouml</td>\n",
       "      <td>0.923124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>columba-1.4-src</td>\n",
       "      <td>0.864979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emf-2.4.1</td>\n",
       "      <td>0.491525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hibernate-distribution-3.3.2.GA</td>\n",
       "      <td>0.836777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jEdit-4.2</td>\n",
       "      <td>0.587699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jfreechart-1.0.19</td>\n",
       "      <td>0.824950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jruby-1.4.0</td>\n",
       "      <td>0.909357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sql12</td>\n",
       "      <td>0.702065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Average</td>\n",
       "      <td>0.746824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Project  F1 Score\n",
       "0                  apache-ant-1.7.0  0.526316\n",
       "1                apache-jmeter-2.10  0.801444\n",
       "2                           argouml  0.923124\n",
       "3                   columba-1.4-src  0.864979\n",
       "4                         emf-2.4.1  0.491525\n",
       "5   hibernate-distribution-3.3.2.GA  0.836777\n",
       "6                         jEdit-4.2  0.587699\n",
       "7                 jfreechart-1.0.19  0.824950\n",
       "8                       jruby-1.4.0  0.909357\n",
       "9                             sql12  0.702065\n",
       "10                          Average  0.746824"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the F1 score over projects in Maldonado62k dataset and calculate the average across 10 projects\n",
    "from sklearn.metrics import f1_score\n",
    "df_pred = pd.read_csv('Paper_Results/Maldonado62k-ICL/Maldonado62k_Input-ct_flan-t5-xxl_ICL-task-MAT_pred.csv')\n",
    "print(len(df_pred))\n",
    "print(df_pred.columns)\n",
    "\n",
    "f1_scores = {}\n",
    "\n",
    "for project_name, group in df_pred.groupby('project'):\n",
    "    real = group['real']\n",
    "    pred = group['pred']\n",
    "    f1_scores[project_name] = f1_score(real, pred, pos_label='SATD', average='binary')\n",
    "    \n",
    "\n",
    "df_f1_scores = pd.DataFrame(list(f1_scores.items()), columns=['Project', 'F1 Score'])\n",
    "df_f1_scores.loc[len(df_f1_scores)] = ['Average', df_f1_scores['F1 Score'].mean()]\n",
    "\n",
    "df_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llm_py310_torch2]",
   "language": "python",
   "name": "conda-env-llm_py310_torch2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
